# infrastructure, design, code quality
  - write a README.md backwards
  - clean up mappers for single use and mappers for repeated use

  - define conventions to get data about arguments to a function
    Arguments.Signature

  - use NODE_PATH or equivalent to set test/*.spec to root
  - move /files handler to /generators/files
  - integration tests that fail should show stderr

# data
  - can a graph have no edges? and only have nodes?
    what does mapping an Array of strings to a graph mean?
      nodes with literals but no edges

# server
  - add ?expand or /expands/<url> to redirect (302) with rewrites expanded, to troubleshoot/modify
  - support ?help or perhaps /help/ (might even be a rewrite rule)
    - list templates with substitutions containing a string
 
# transform
  - [x] clean up use of Args in handlers
  - filter (eg. /files/data/people/transform/filter-out,directories )
  - head/tail, for x milliseconds
  - extract rows from html table to scrape from web page
  - reduce to a set (e.g. transactions.csv/reduce/table/account-names/reduce/set )
  - order by last modified
  - order by most used
  - `get` use cat or curl or open depending on type
  - `echo google.com | transform response`.  transforms are functions that all input and can generate multiple outputs
  - research how to convert an object into a variety of outputs.
    - consume a graph (from csv, ttl) to node-edges or triples or (p(s,o))
      or should we just do `reduce graph.triples`
  - interpret input (with hints as to nature) then push results onto output
  - see http://highlandjs.org/#consume

  - $ get people | reduce graph | transform triples

  f='./test.txt'
  stream([f]).map(content).reduce(graph).transform(graph.nodeEdges)

  perhaps relates to the concept of type hierarchy (starting to take form in search?)
  - reduce accepts n items or n seconds worth
  - use "reduce one" to display a selector and let the user pick on
  - "reduce join, sort, unique" while retaining context so that you can get a list of unique items, while retaining where they came from. eg: "files duration:1000 data/people | map content  | sort  -u" replaced with "... | map augment content | parse content | reduce join sort unique"
  - join streams, so that I can merge two resources together, so that I can merge remote resources
  - replace .value with .match
  - build mapper to replace (.match, .path) with .value
  - integrate todo into software engineering process

# search
  - bookmarks
  - search for directories from command line
  - interface for searchers
    - file/network searcher is a stream
    - in-memory data can be searched too
      - why return a stream when we can return the entire object in memory
    - an in-memory object is data, a Map, has entries
    - a value of a map can be a searcher
    - when a value is a stream, match stream could return a stream as the match.value

    - query.match stream
      - what if the data is a stream
      - a match can be a stream

  - how to express `search ... where ...`
  - search within content (file or response)
    - filter results from context queries (eg file = search context.source file "$@") to replace defining recursion level
      - you care to filter results based on who generated them, not only what they contain (which could be faked)
    - honer defaultPathQuery to avoid traversing directories (following references)
  - search ^/<dir> should only search in that directory?
  - order by custom order in .search.order in each directories
  - search in . before other directories
    - use inode map to avoid searching in . twice
  - support case insensitive search
  - interactively pick one of several if multiple
  - order search results
    - order files based on "relevance" (eg. a query "foo" should first return names matching query entirely: ["foo", "abcfoodef"])
  - store search results in temporary files to be able to access the content again

# visualize
  - show a graph inside a node
  - time series
